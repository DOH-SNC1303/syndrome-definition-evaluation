---
output: html_document
---

<!-- Setup -->

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)

load("Resources\\Consensus_Review_Parameters.RData")

pacman::p_load(irr,
               readxl,
               tidyverse,
               writexl)

definition <- str_extract(string = filepath, pattern = "\\([:alpha:]{1,}\\)") %>% 
  str_remove_all(string =., pattern = '[:punct:]') # (Corresponding def#_name is extracted from filepath.)

```

```{r Custom Functions}

## Cut Review Scale into Categories

cut_review_scale <- function(scale_low, scale_high){
  
  rating_scale <- seq(from = scale_low, to = scale_high, by = 1) # Create rating scale vector (of integers)
  
  scale_length <- length(rating_scale) %% 2 # 0 remainder for even, 1 remainder for odd
  
  n_chunks <- ifelse(scale_length == 0, 2, 3) # Determine the number of chunks to cutting the rating scale along to apply qualitative categories.
  
  rating_categories <- split(rating_scale, cut(seq_along(rating_scale), n_chunks)) # Source: https://www.geeksforgeeks.org/split-vector-into-chunks-in-r/#
  
  ## Assign List Element Names Detailing the Scale Category ##
  
  if(n_chunks == 2){
    
    names(rating_categories) <- c("False Positive", "True Positive")
    
  }else if(n_chunks == 3){
    
    names(rating_categories) <- c("False Positive", "Uncertain", "True Positive")
  }
  
  return(rating_categories)
}

## Assign Category labels to data frame

assign_review_label <- function(df, review_scale_low = ReviewScaleLow, review_scale_high = ReviewScaleHigh){
  
  # Create Rating Categories
  rating_categories <- cut_review_scale(scale_low = review_scale_low, scale_high = review_scale_high)
  
  # Assign Rating Category Labels
  if(length(rating_categories) == 2){ # If review_scale was even, only FP/TP (No Uncertain)
    
    df_labelled <- df %>%
      mutate(Reviewer_TP_Label = case_when(
        get(paste0("Reviewer_TP_",review_scale_low, review_scale_high)) %in% rating_categories[["False Positive"]] ~ "False Positive",
        get(paste0("Reviewer_TP_",review_scale_low, review_scale_high)) %in% rating_categories[["True Positive"]] ~ "True Positive"))
    
  }else if(length(rating_categories) == 3){ # If review_scale was odd, FP/Uncertain/TP
    
    df_labelled <- df %>%
      mutate(Reviewer_TP_Label = case_when(
        get(paste0("Reviewer_TP_",review_scale_low, review_scale_high)) %in% rating_categories[["False Positive"]] ~ "False Positive",
        get(paste0("Reviewer_TP_",review_scale_low, review_scale_high)) %in% rating_categories[["Uncertain"]] ~ "Uncertain",
        get(paste0("Reviewer_TP_",review_scale_low, review_scale_high)) %in% rating_categories[["True Positive"]] ~ "True Positive"))
  }
  
  # Return labelled dataframe to global environment
  return(df_labelled)
}

```

```{r Load in All Reviewer Data}

list_reviewer <- list()

for(i in 1:n_reviewers){

list_reviewer[[i]] <- readxl::read_excel(path = paste0("1_Reviewed_Data\\Reviewer_",i,"\\Reviewer_",i,"_Data.xlsx")) %>%
  assign_review_label(df=.)

names(list_reviewer)[[i]] <- paste0("Reviewer_",i)
}

```

```{r Join All Reviewer Data}

# Full Join All Reviewer Data Frames

# Note: Each data frame will have added numeric suffixes added to their review columns. These suffixes can be used to identify individual reviewer scores once they are all joined together. (Ex: We have 3 reviewers using a 1-5 scale, we will join all their review DFs into one DF. Review columns will be: Reviewer_TP_15_[1-3], Reviewer_Notes_[1-3], and Reviewer_TP_Label_[1-3])

comparison <- list_reviewer %>% reduce(full_join,
                                   by = c("Row_Number","Date", "EssenceID", select_fields),
                                   suffix = paste0("_",1:(length(list_reviewer)-1))) %>%
  ### Applies suffix to final joined DF as reduce() does not apply it. ###
  rename(!!paste0("Reviewer_TP_",ReviewScaleLow,ReviewScaleHigh,"_",n_reviewers) := !!paste0("Reviewer_TP_",ReviewScaleLow,ReviewScaleHigh),
         !!paste0("Reviewer_Notes_",n_reviewers) := "Reviewer_Notes",
         !!paste0("Reviewer_TP_Label_",n_reviewers) := "Reviewer_TP_Label") %>%
  ## Reorder Variables ##
  select(Row_Number, Date, EssenceID,
         matches("[0-9]{2}\\_[0-9]{1,}"), ## Reviewer_TP_## variables
         matches("Label\\_[0-9]{1,}"), # Reviewer_TP_Label variables
         matches("Notes\\_[0-9]{1,}"), # Reviewer_Notes variables
         everything()) # This reordering is needed to create the Agreement variable.


```

```{r Detect Agreements and Disagreements}

comparison <- comparison %>%
  rowwise() %>%
  mutate(Agreement =
           all(c_across(Reviewer_TP_Label_1:!!(paste0("Reviewer_TP_Label_",n_reviewers))) ==
                 first(c_across(Reviewer_TP_Label_1:!!(paste0("Reviewer_TP_Label_",n_reviewers))))), # Agreement assess the IRR of Qualitative Review Categories (FP/Uncertain/TP),
         Date = as.Date(Date)) %>% 
  ### Reorder variables ###
  select(Row_Number, Date, EssenceID, Agreement, everything())

```

<!-- Write Data -->

```{r Write Comparison Datasets}

# All
comparison %>%
  arrange(Agreement) %>%
  writexl::write_xlsx(x=., path = paste0("2_Comparison_Data\\Comparison_Data_All.xlsx"))

# Disagreements Only
comparison %>%
  filter(Agreement == FALSE) %>%
  writexl::write_xlsx(x=.,path = paste0("2_Comparison_Data\\Comparison_Data_Disagreements.xlsx"))

```

```{r Write Consensus Dataset}

comparison %>%
  mutate(Consensus = case_when(
    Agreement == TRUE ~ Reviewer_TP_Label_1,
    TRUE ~ NA),
    Consensus_Edits = NA) %>%
  select(Row_Number, Date, EssenceID, Agreement, Consensus, Consensus_Edits, everything()) %>%
  writexl::write_xlsx(x=.,path = paste0("3_Consensus_Data\\Consensus_Data.xlsx"))

```

---
title: "Consensus Review: `r definition`"
---

**Jurisdiction**: `r jurisdiction`  
**Report Created**: `r Sys.Date()`    
**Point of Contact**: `r DefinitionInformation[["Setup"]]$PointOfContact` (`r DefinitionInformation[["Setup"]]$POCEmail`)    
**Organization**: `r DefinitionInformation[["Setup"]]$Organization`

<br />

# Instructions

After rendering this R Markdown report, review scale ratings assigned by the `r n_reviewers` participating reviewers will be compared to assess agreement and disagreement for each reviewed record. Comparison files will be available under **2_Comparison_Data**. 

When you are ready to discuss and come to a consensus decision about records with disagreements between the `r n_reviewers` participating reviewers, use: **3_Consensus_Data/Consensus_Data.xlsx**. Within this file, fill in the **Consensus** variable after a decision has been made, and under the **Consensus_Edits** variable you may write a note for which reviewer(s) switched their initial rating as well as the supporting information that led to the consensus decision.

After completing the Consensus Review process, **Consensus_Data.xlsx** may be utilized to assess a definition's performance (Sensitivity, PPV, Specificity, NPV). 

<br />

# Assessing Interrater Reliability  {.tabset}

Examining the initial agreement between the `r n_reviewers` participating reviewers (before Consensus Review) can provide some insight into the reliability of the reviewer metrics as well as how difficult the reviewed definition was to classify. 

```{r IRR setup}

comparison_IRR <- comparison %>%
  select(matches("[0-9]{2}\\_[0-9]{1,}")) # Include only the review columns with assigned values on the rating scale.

ReviewScale <- cut_review_scale(scale_low = ReviewScaleLow, scale_high = ReviewScaleHigh)
  
```

## Percent Agreement: Review Categories

This section examines the level of agreement across the **broad, qualitative review categories** applied to each record (typically what we care most about). For this validation review, the review categories included: **`r names(ReviewScale)`**.

```{r Qual Review Category Agreement}

n_agree <- comparison %>% filter(Agreement == TRUE) %>% nrow()
n_sample <- comparison %>% nrow()

pcnt_agree <- round((n_agree/n_sample)*100,1)
pcnt_disagree <- round(((n_sample-n_agree)/n_sample)*100,1)

cat("Interrater Agreement:\n Records: ",n_sample,"\n Raters: ", n_reviewers,"\n %-agree: ", pcnt_agree)

```

## Percent Agreement: Review Scales

This section examines the level of agreement across the **specific, quantitative review scale rating** applied to each record. For this validation review, the review scale ratings ranged from `r ReviewScaleLow` to `r ReviewScaleHigh` and were sorted into review categories using the following schema: 

```{r Print ReviewScale}
print(ReviewScale)
```

```{r Quant Review Scale Agreement}

comparison_IRR %>% agree(.)

```

`r if(n_reviewers == 2){paste0('## ','Cohens Kappa: Review Scales')}`
`r if(n_reviewers > 2){paste0('## ','Lights Kappa: Review Scales')}`

Although Percent Agreement is a helpful and simple metric to quickly examine Interrater Reliability, it has a notable weakness. Percent Agreement does not adjust for the level of agreement that would be expected by random chance, and therefore overestimates the level of agreement between reviewers.The [Kappa statistic](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3402032/) is utilized to adress this weakness and can have values that range from 1 to -1. [Guidelines for examining Kappa values have suggested the following interpretation (although this may differ by use case):]{.underline}

* **-1 to -0.1**: Agreement is worse than random (active disagreement)
* **0 to 0.2**: Slight Agreement
* **0.21 to 0.4**: Fair Agreement
* **0.41 to 0.60**: Moderate Agreement
* **0.61 to 0.80**: Substantial Agreement
* **0.81 to 1.0**: Almost Perfect or Perfect Agreement


```{r Kappa Intro, results='asis'}

if(n_reviewers == 2){
  
  cat("The Kappa statistic presented below is **Cohen's Kappa statistic** which was developed to assess interrater reliability between **2** reviewers.")
  
}else if(n_reviewers > 2){
  
    
  cat("The Kappa statistic presented below is **Light's Kappa statistic** developed to assess interrater reliability between **3+** reviewers. Specifically, Light's Kappa statistic is calculated by taking the average of the Cohen's Kappa stastic between all possible 2 reviewer pairs.")
  
}

```

```{r Quant Review Scale Kappa, warning = FALSE}

# 3) Cohen's & Light's Kappa
# Source: (Discussing appropriate use of IRR statistics) https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3402032/

if(n_reviewers == 2){ # Cohen's original Kappa only appropriate for 2 reviewers

  comparison_IRR %>% kappa2(.)

}else if(n_reviewers > 2){ # Light's Kappa - Average of all possible combinations of 2x2 Cohen Kappa's

  comparison_IRR %>% kappam.light(.)
}

```
