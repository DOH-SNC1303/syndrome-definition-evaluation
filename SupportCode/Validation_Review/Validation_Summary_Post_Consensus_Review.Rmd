---
output: 
  html_document:
    toc: true
    toc_float: true
---

<!-- Setup -->

```{r Custom Functions, echo=FALSE}

## Function 1: Pull Non-NA Parameter Values from DefinitionInformation

pull_no_na <- function(df, variable){
  
  df_pulled <- df %>%
    filter(!is.na(get(variable))) %>% # Remove All NA rows for the variable (likely unfilled EXCEL rows)
    pull(get(variable)) # Get the value of the variable
  
  return(df_pulled)
}

## Function 2a: Cut Review Scale into Categories (function used inside 2b)

cut_review_scale <- function(scale_low, scale_high){
  
  rating_scale <- seq(from = scale_low, to = scale_high, by = 1) # Create rating scale vector (of integers)
  scale_length <- length(rating_scale)
  n_chunks <- ifelse((scale_length %% 2) == 0, 2, 3)  # scale_length %% 2 is 0 for even length scales and 1 for odd length scales

  rating_categories <- list()
  
  if(n_chunks == 2){ # FP = scale_low to first half of rating_scale; TP = second half of rating_scale to scale_high
    
    rating_categories[["False Positive"]] <- seq(from = scale_low, to = rating_scale[(scale_length/2)], by = 1)
    rating_categories[["True Positive"]] <- seq(from = rating_scale[((scale_length/2))+1], to = scale_high, by = 1)
    
  }else if(n_chunks == 3){ # FP = scale_low to 1 before median of rating_scale; Uncertain = median value of rating scale; TP = 1 after median of rating scale to scale_high. 
    
    rating_categories[["False Positive"]] <- seq(from = scale_low, to = (median(rating_scale) - 1), by = 1)
    rating_categories[["Uncertain"]] <- median(rating_scale)
    rating_categories[["True Positive"]] <- seq(from = (median(rating_scale) + 1), to = scale_high, by = 1)
  }
  
  return(rating_categories)
}

## Function 2b: Assign Category labels to data frame

assign_review_label <- function(df, review_scale_low = ReviewScaleLow, review_scale_high = ReviewScaleHigh){
  
  # Create Rating Categories
  rating_categories <- cut_review_scale(scale_low = review_scale_low, scale_high = review_scale_high)
  
  # Assign Rating Category Labels
  if(length(rating_categories) == 2){ # If review_scale was even, only FP/TP (No Uncertain)
    
    df_labelled <- df %>%
      mutate(Review_Category = case_when(
        Review_Rating %in% rating_categories[["False Positive"]] ~ "False Positive",
        Review_Rating %in% rating_categories[["True Positive"]] ~ "True Positive"))
    
  }else if(length(rating_categories) == 3){ # If review_scale was odd, FP/Uncertain/TP
    
    df_labelled <- df %>%
      mutate(Review_Category = case_when(
        Review_Rating %in% rating_categories[["False Positive"]] ~ "False Positive",
        Review_Rating %in% rating_categories[["Uncertain"]] ~ "Uncertain",
        Review_Rating %in% rating_categories[["True Positive"]] ~ "True Positive"))
  }
  
  # Return labelled data frame to global environment
  return(df_labelled)
}

## Function 3: Prepare data for caret::confusionMatrix()

prep_cm <- function(df, use_case){
  
  df_prepped <- df %>%
    mutate(Definition = "Positive", # Prediction Variable (aka whether the query tagged/identified the record) --> Will always be 1. 
         Definition = factor(Definition, levels = c("Positive", "Negative")))
  
  if(use_case == "reviewer"){
    
    df_prepped <- df_prepped %>%
      mutate(across(
        .cols = matches("^Review_Category"),
        .fns = ~factor(.x, 
                       levels = c("True Positive", "False Positive"), 
                       labels = c("Positive", "Negative"))))
    
  }else if(use_case == "consensus"){
    
    df_prepped <- df_prepped %>%
      mutate(Review_Category_Consensus = factor(Review_Category_Consensus, # caret requires ordered factors 
                                      levels = c("True Positive", "False Positive"), 
                                      labels = c("Positive", "Negative"))) # shorten factor labels
  }

return(df_prepped)
}

## Function 4: Modified caret::confusionMatrix()

confusion_matrix <- function(df, use_case){
  
  # Step 0: Generate storage list
  list_storage <- list()
  
  # Step 1: Generate caret::confusionMatrix()
  if(use_case == "reviewer"){
     CM <- caret::confusionMatrix(data = df$Definition, # Query detected record?
                               reference = df$Review_Category) # Query classification accurate?  
     
  }else if(use_case == "consensus"){
     CM <- caret::confusionMatrix(data = df$Definition, reference = df$Review_Category_Consensus) 
  }

  # Step 2: Extract caret 2x2 table from CM
  list_storage[["table"]] <- CM$table
  
  # Step 3: Extract PPV & Accuracy (95% CI) from CM
  list_storage[["metrics"]] <- broom::tidy(CM) %>%
    filter(term %in% c("accuracy", "pos_pred_value")) %>% # Only metrics we want to keep
    mutate(across(.cols = where(is.numeric), .fns = ~ round(.x, digits = 2))) %>% # Round metrics to 2 decimals
    mutate(value = ifelse(!is.na(conf.low) & !is.na(conf.high),
                          paste0(estimate," (",conf.low,", ",conf.high,")"), estimate), # Combine CI w/estimate
           term = case_when( # Relabel terms/metrics
             term == "pos_pred_value" ~  "positive predictive value (PPV)",
             term == "accuracy" ~ "accuracy (95% CI)",
             TRUE ~ term)) %>%
    arrange(desc(term)) %>% # Display PPV first then Accuracy. 
    select(metric = term, value) %>%
    as.data.frame() # Convert to DF to allow printing. 
  
  
  if(use_case == "consensus"){
    print(list_storage)
  }
  
  return(list_storage)
}
```

```{r Setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)

load("Resources\\Validation_Review_Parameters.RData")

pacman::p_load(caret,
               gtsummary,
               irr,
               readxl,
               tidyverse,
               writexl)

### Recreate Needed Evaluation_[X]Defs Parameters from DefinitionInformation

# Number of Validation Reviewers
n_reviewers <- DefinitionInformation[["ValidationReviewInformation"]]$ReviewerID %>% max()

# ESSENCE fields included
select_fields <- DefinitionInformation[["AppliedFields"]] %>% pull_no_na(df=., variable = "Field")

# Jurisdiction of Validation Reviewers
jurisdiction <- DefinitionInformation[["Setup"]]$Jurisdiction

# Review Scale: High/Low
ReviewScaleLow <- DefinitionInformation[["ValidationReviewInformation"]]$ReviewScaleLow[1]
ReviewScaleHigh <- DefinitionInformation[["ValidationReviewInformation"]]$ReviewScaleHigh[1]
ReviewScale <- cut_review_scale(scale_low = ReviewScaleLow, scale_high = ReviewScaleHigh)

# Extract Defintion Undergoing Validation Review 
definition <- str_extract(string = filepath, pattern = "\\([:alpha:]{1,}\\)") %>% 
  str_remove_all(string =., pattern = '[:punct:]') # (Corresponding def#_name is extracted from filepath.)

```

```{r Load in All Reviewer Data}

list_reviewer <- list()

for(i in 1:n_reviewers){

list_reviewer[[i]] <- readxl::read_excel(path = paste0("1_Reviewed_Data\\Reviewer_",i,"\\Reviewer_",i,"_Data.xlsx")) %>%
  assign_review_label(df=.)

names(list_reviewer)[[i]] <- paste0("Reviewer_",i)
}

# Save for Confusion Matrix Generation (line ~460)
list_reviewer_cm <- list_reviewer 

```

```{r Join All Reviewer Data}
if(n_reviewers > 1){
  
# Full Join All Reviewer Data Frames

# Note: Each data frame will have added (_R#) suffixes added to their review columns. These suffixes can be used to identify individual reviewers once all of their review variables are joined together. (Ex: We have 3 reviewers you will see Review_Ratings_R[1-3], Review_Categories_R[1-3], Notes_R[1-3]).
  
# Step 1: Assign Reviewer ID (_R#) Suffixes to Each Reviewer Variable (to allow us to distinguish when all ratings are joined together)

for(i in seq_along(list_reviewer)){
  
  list_reviewer[[i]] <- list_reviewer[[i]] %>%
    rename_with(.data =.,
                .cols = c(Review_Rating, Review_Category, Notes),
                .fn = ~paste0(.,"_R",i))
}
  
# Step 2: Join All Reviewer DFs together and reorder variables
  
comparison <- list_reviewer %>%
  reduce(full_join, by = c("Date", "EssenceID", select_fields)) %>%
  select(Date, EssenceID,
         matches("Review\\_Rating"), ## Review_Rating(_R#) variable
         matches("Review\\_Category"), # Reviewer_Category(_R#) variable
         matches("^Notes"), # Notes(_R#) variable
         everything()) # This reordering is needed to create the Agreement variable.

}
```

```{r Detect Agreements and Disagreements}
if(n_reviewers > 1){
  
comparison <- comparison %>%
  rowwise() %>%
  mutate(Agreement =
           all(c_across(Review_Category_R1:!!(paste0("Review_Category_R",n_reviewers))) ==
                 first(c_across(Review_Category_R1:!!(paste0("Review_Category_R",n_reviewers))))), # Agreement assess the IRR of Qualitative Review Categories (FP/Uncertain/TP),
         Date = as.Date(Date)) %>% 
  ### Reorder variables ###
  select(Date, EssenceID, Agreement, everything())

}
```

<!-- Write Data -->

```{r Write Consensus Dataset}
if(n_reviewers > 1){
 
  comparison %>%
  arrange(Agreement) %>% # Disagreements at the top
  mutate(Review_Category_Consensus = case_when(
    Agreement == TRUE ~ Review_Category_R1,
    TRUE ~ NA),
    Notes_Consensus = ifelse(Agreement == TRUE, "-", NA)) %>%
  select(Date, EssenceID, Agreement, Review_Category_Consensus, Notes_Consensus, everything()) %>%
  writexl::write_xlsx(x=.,path = paste0("2_Consensus_Data\\Consensus_Data.xlsx")) 
  
}
```

---
title: "Validation Summary (Pre Consensus Review): `r definition`"
---

**Jurisdiction**: `r jurisdiction`  
**Report Created**: `r Sys.Date()`    
**Point of Contact**: `r DefinitionInformation[["Setup"]]$PointOfContact` (`r DefinitionInformation[["Setup"]]$POCEmail`)    
**Organization**: `r DefinitionInformation[["Setup"]]$Organization`


```{r Instructions Text, results= 'asis'}
if(n_reviewers == 1){
  
  cat(paste0(
    '# Instructions\n',
    'After rendering this R Markdown report, query performance metrics will be generated below.(Note: Records assigned the **Uncertain** review category will not be included in query performance metric calculations). There is no need to run **Validation_Summary_Post_Consensus_Review.Rmd** unless you wish to conduct an additional validation review cycle with multiple participating reviewers.',
    '<br> <br>'
  ))
  
}else if(n_reviewers > 1){
  
  cat(paste0(
    '# Consensus Review Instructions\n',
    'After rendering this R Markdown report, validation reviews by each of the participating reviewers will be compared to assess agreement and disagreement for each reviewed record. Additionally, query performance metrics (by reviewer) and interrater reliability metrics will be generated below. These metrics have been provided to assess how closely aligned participants were in their validation reviews and address any issues that may have emerged during the validation review process. (Note: Records assigned the **Uncertain** review category will not be included in query performance metric calculations). \n',
    'When you are ready to discuss validation review findings and come to a consensus decision among the participating reviewers (particularly for records where there were disagreements), access the following file:  **2_Consensus_Data/Consensus_Data.xlsx**.\n',
    '\n',
    '[Within **Consensus_Data.xlsx**:]{.underline}\n',
    '\n',
    '- Discuss records where there are disagreements between the participating reviewers (i.e. records where the **Agreement** variable will be **FALSE**).\n',
    '- After a consensus decision has been made, fill in the **Review_Category_Consensus** variable with the agreed upon review category.\n',
    '- (*Optional; if helpful*) Fill out the **Notes_Consensus** to explain which reviewer(s) changed their review category and rating upon coming to a consensus decision. This documentation will provide clarity as to how disagreements were resolved and what pieces of information may be useful to informing final record classifications\n',
    '\n',
    'After completing the consensus review process and running **Validation_Summary_Post_Consensus_Review.Rmd**, query performance metrics (utilizing consensus review categories) will be generated. Additionally, **Consensus_Data.xlsx** will be copied and subset to remove the no longer needed *Reviewer* variables. This data will be called **Consensus_Data_Subset.xlsx**.',
    '<br> <br>'
    ))
}

```

```{r Assessing Interrater Reliability Text, results = 'asis'}
if(n_reviewers > 1){
  cat(
    paste0('# Assessing Interrater Reliability  {.tabset}\n',
           'Examining the initial agreement between the participating reviewers (**before Consensus Review**) can provide some insight into the reliability of the reviewer metrics as well as how difficult the reviewed definition was to classify. ')
    )
}
```

```{r Reviewer Distributions Setup}
if(n_reviewers > 1){

list_reviewer_comparison <- list()

# To Showcase Distribution of Review Categories and Review Scales by Rater
list_reviewer_comparison[["rating_categories"]] <- comparison %>%
  select(EssenceID, matches("^Review_Category")) %>%
  pivot_longer(data =., 
               cols = contains("Review_Category"),
               names_to = "Reviewer",
               values_to = "Rating_Categories") %>%
  mutate(Reviewer = str_replace_all(Reviewer, pattern = "Review_Category_R", replacement = "Reviewer "))

list_reviewer_comparison[["rating_scales"]] <- comparison %>%
  select(EssenceID, matches("^Review_Rating")) %>%
  pivot_longer(data =., 
               cols = contains("Review_Rating"),
               names_to = "Reviewer",
               values_to = "Rating_Scale") %>%
  mutate(Reviewer = str_replace_all(Reviewer, pattern = "Review_Rating_R", replacement = "Reviewer "))


list_reviewer_comparison[["distributions"]] <- list_reviewer_comparison[["rating_categories"]] %>%
  left_join(., list_reviewer_comparison[["rating_scales"]], by = join_by("EssenceID", "Reviewer")) %>%
  mutate(Rating_Scale = factor(Rating_Scale,
                                 levels = seq(from = ReviewScaleLow, to = ReviewScaleHigh, by = 1)))

if(length(ReviewScale) == 2){
  
list_reviewer_comparison[["distributions"]] <- list_reviewer_comparison[["distributions"]] %>%
    mutate(Rating_Categories = factor(Rating_Categories, levels = c("False Positive", "True Positive")))
  
}else if(length(ReviewScale) == 3){
  
list_reviewer_comparison[["distributions"]] <- list_reviewer_comparison[["distributions"]] %>%
    mutate(Rating_Categories = factor(Rating_Categories, levels = c("False Positive", "Uncertain","True Positive")))
  
}

}
```

```{r Reviewer Distributions Text, results = 'asis'}
if(n_reviewers > 1){

cat(
    paste0('## Reviewer Distributions\n',
           'This sections examines the overall distributions of review categories (qualitative labels) and review ratings (quantitative values) applied by each reviewer.'
    )
  )
  
}
```

```{r Reviewer Distributions}
if(n_reviewers > 1){

list_reviewer_comparison[["distributions"]] %>% 
  select(-EssenceID, `Rating Categories` = Rating_Categories, `Rating Scale` = Rating_Scale) %>%
  tbl_summary(by = Reviewer,
              missing_text = "Missing") %>%
  add_p() %>%
  add_overall(last=TRUE) %>%
  modify_header(label ~ "") %>%
  bold_labels()

}
```

```{r, results='asis'}
if(n_reviewers > 1){
  cat(
    paste0('<br> <br>')
    )
}
```

```{r IRR Setup}
if(n_reviewers > 1){
 
# Generate Storage List
list_reviewer_IRR <- list()

# To Generate IRR Metrics

comparison_IRR <- comparison %>%
  select(matches("^Review_Rating")) # Include only the review columns with assigned values on the rating scale, removes other variables (ex: Review_TP_15_[REVIEWER#]). 
  
}
```

```{r Percent Agreement Review Categories Text, results = 'asis'}
if(n_reviewers > 1){
  cat(
    paste0('## Percent Agreement: Review Categories\n',
           'This section examines the level of agreement across the **broad, qualitative review categories** applied to each record. For this validation review, the following review categories were utilized: ')
  )
}
```

```{r Print Review Categories}
if(n_reviewers > 1){

print(names(ReviewScale))

}
```

```{r Percent Agreement Review Categories}
if(n_reviewers > 1){

n_agree <- comparison %>% filter(Agreement == TRUE) %>% nrow()
n_sample <- comparison %>% nrow()
pcnt_agree <- round((n_agree/n_sample)*100,1)

list_reviewer_IRR[["Percent Agreement: Review Categories"]] <- paste0("Interrater Agreement:\n Records: ",n_sample,"\n Raters: ", n_reviewers,"\n %-agree: ", pcnt_agree)

cat(list_reviewer_IRR[["Percent Agreement: Review Categories"]]) 
  
}
```

```{r, results='asis'}
if(n_reviewers > 1){
  cat(
    paste0('<br> <br>')
    )
}
```

```{r Percent Agreement Review Ratings Text, results = 'asis'}
if(n_reviewers > 1){
  cat(
    paste0('## Percent Agreement: Review Ratings\n',
           'This section examines the level of agreement across the **specific, quantitative review ratings** applied to each record. For this validation review, review ratings were nested within the review categories using the following schema: ')
  )
}
```

```{r Print ReviewScale}
if(n_reviewers > 1){
  
  print(ReviewScale)
}
```

```{r Percent Agreement Review Scales}
if(n_reviewers > 1){

(list_reviewer_IRR[["Percent Agreement: Review Ratings"]] <- comparison_IRR %>% irr::agree(.))

}
```

```{r, results='asis'}
if(n_reviewers > 1){
  cat(
    paste0('<br> <br>')
    )
}
```

`r if(n_reviewers == 2){paste0('## ','Cohens Kappa: Review Scales')}`
`r if(n_reviewers > 2){paste0('## ','Lights Kappa: Review Scales')}`

```{r Kappa Review Scales Text, results='asis'}
if(n_reviewers > 1){
  
  cat(
    paste0("Although Percent Agreement is a simple metric to quickly examine Interrater Reliability, it has a notable weakness. **Percent Agreement does not adjust for the level of agreement that would be expected by random chance, and therefore overestimates the level of agreement between reviewers**. The [Kappa statistic](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3402032/) is utilized to adress this weakness and can have values that range from 1 to -1. [Guidelines for examining Kappa values have suggested the following interpretation (although this may differ by use case):]{.underline}

* **-1 to -0.1**: Agreement is worse than random (active disagreement)
* **0 to 0.2**: Slight Agreement
* **0.21 to 0.4**: Fair Agreement
* **0.41 to 0.60**: Moderate Agreement
* **0.61 to 0.80**: Substantial Agreement
* **0.81 to 1.0**: Almost Perfect or Perfect Agreement"
  ))
  
}
```

```{r Kappa Review Scales Description, results='asis'}
if(n_reviewers == 2){
  
  cat("The Kappa statistic presented below is **Cohen's Kappa statistic** which was developed to assess interrater reliability between **2** reviewers.")
  
}else if(n_reviewers > 2){
  
  cat("The Kappa statistic presented below is **Light's Kappa statistic** developed to assess interrater reliability between **3+** reviewers. Specifically, Light's Kappa statistic is calculated by taking the average of the Cohen's Kappa stastic between all possible 2 reviewer pairs.")
  
}

```

```{r Quant Review Scale Kappa, warning = FALSE}

# 3) Cohen's & Light's Kappa
# Source: (Discussing appropriate use of IRR statistics) https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3402032/

if(n_reviewers == 2){ # Cohen's original Kappa only appropriate for 2 reviewers

  (list_reviewer_IRR[["Cohen's Kappa"]] <- comparison_IRR %>% irr::kappa2(.))

}else if(n_reviewers > 2){ # Light's Kappa - Average of all possible combinations of 2x2 Cohen Kappa's

  (list_reviewer_IRR[["Light's Kappa"]] <- comparison_IRR %>% irr::kappam.light(.))
}

```

```{r, results='asis'}
if(n_reviewers > 1){
  cat(
    paste0('<br> <br>')
    )
}
```

# Query Performance (by Reviewer) {.tabset}

```{r Query Performance by Reviewer}

# Step 1: Prepare Data for Confusion Matrix

for(i in 1:n_reviewers){
  
  list_reviewer_cm[[i]] <- list_reviewer_cm[[i]] %>% 
    prep_cm(df=., use_case = "reviewer")
}

# Step 2: Dynamically Render Query Performance by Review Tabbed Widgets using list_reviewer. SOURCE: https://stackoverflow.com/questions/42631642/creating-dynamic-tabs-in-rmarkdown

if(length(ReviewScale) == 2){ # TP, FP
  
  query_performance_template <- c(
    "```{r, echo=FALSE, results='asis'}\n",
    "cat(
    paste0('## ', 'Reviewer ', {{i}},' (',DefinitionInformation[['ValidationReviewInformation']]$ReviewerName[{{i}}],')','\n'))",
    "```\n",
    "\n",
    "```{r}\n", # Initializes R code to create 1) Tabbed Widget and 2) Insert ES Text.
    "list_reviewer_cm[[{{i}}]] %>% confusion_matrix(df=., use_case = 'reviewer')", # Print confusion matrix
    "```\n", # Ends R Code and adds a space (to differentiate tabbed widgets)
    "\n")
  
}else if(length(ReviewScale) == 3){ # TP, Uncertain, FP
  
    query_performance_template <- c(
    "```{r, echo=FALSE, results='asis'}\n",
    "cat(
    paste0('## ', 'Reviewer ',{{i}},' (', DefinitionInformation[['ValidationReviewInformation']]$ReviewerName[{{i}}],')','\n'),
    paste0('**Note: ** Records with the review category of **Uncertain** are excluded from query performance calculations.', '\n'))",
    "```\n",
    "\n",
    "```{r}\n", # Initializes R code to create 1) Tabbed Widget and 2) Insert ES Text.
    "list_reviewer_cm[[{{i}}]] %>% confusion_matrix(df=., use_case = 'reviewer')", # Print confusion matrix
    "```\n", # Ends R Code and adds a space (to differentiate tabbed widgets)
    "\n")
}

reviewer_query_performance <- lapply(seq_along(list_reviewer), function(i) {knitr::knit_expand(text= query_performance_template)}) # Use query_performance_template above, and render it via knitr::knit_expand()

```

`r knitr::knit(text = unlist(reviewer_query_performance))`

```{r Write Reviewer RData File}
if(n_reviewers == 1){
  
    save(list_reviewer, list_reviewer_cm, file = "1_Reviewed_Data\\Reviewer_Data.RData")
  
}else if(n_reviewers > 1){
  
  save(list_reviewer, list_reviewer_comparison, list_reviewer_cm, list_reviewer_IRR, file = "1_Reviewed_Data\\Reviewer_Data.RData")
}
```

```{r, results='asis'}
if(n_reviewers > 1){
  cat(
    paste0('<br> <br>')
  )
}
```

```{r Query Performance Consensus Text, results='asis'}
if(n_reviewers > 1){
  cat(
    paste0('# Query Performance (Consensus) \n',
           'Examining definitions using a consensus review framework may provide less-biased estimates than those of an individual reviewer as multiple backgrounds, skill sets, and perspectives are included in the classification of a record. **Note: ** Records with the review category of **Uncertain** are excluded from query performance calculations.')
  )
}
```

```{r Query Performance Consensus}
if(n_reviewers > 1){
  
  # Generate Storage List
  list_consensus <- list()
  
  # Read in Consensus Data (Post-Consensus Review)
  list_consensus[["Consensus_Data"]] <- readxl::read_excel(path = paste0("2_Consensus_Data\\Consensus_Data.xlsx")) 
  
  # Prepare Consensus Data for Confusion Matrix
  consensus_cm <- list_consensus[["Consensus_Data"]] %>% 
    prep_cm(df=., use_case = "consensus")
  
  # Generate Confusion Matrix
  list_consensus[["Confusion_Matrix"]] <- consensus_cm %>%
    confusion_matrix(df=., use_case = "consensus")
}
```

```{r Write Subset Consensus Data}
if(n_reviewers > 1){
  
  # Generate Consensus Data Clean (Remove Unnecessary Review Variables)
  list_consensus[["Consensus_Data_Subset"]] <- list_consensus[["Consensus_Data"]] %>% 
    arrange(Review_Category_Consensus) %>%
    select(-Agreement, -Notes_Consensus, 
           -matches("^Review_Rating"),
           -matches("^Review_Category\\_R[0-9]?"),
           -matches("^Notes"))  # Remove all Reviewer Variables (except Consensus_Label)
  
  # Write Excel File
  writexl::write_xlsx(list_consensus[["Consensus_Data_Subset"]], path = "2_Consensus_Data\\Consensus_Data_Subset.xlsx")
  
  # Write RData file
  save(list_consensus, file = "2_Consensus_Data\\Consensus.RData")
  
}
```